{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Understanding with Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides insights into building a Code Understanding App using Local LLMs and a RAG architecture. \\\n",
    "*Please notice that the notebook is still being edited.*\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "In the realm of software development, grappling with ill documented or poorly written code is a common challenge that consumes valuable time and resources. \n",
    "\n",
    "Despite source code analysis being one of the most popular LLM applications, traditional web-based interfaces come with their own set of limitations.\n",
    "One primary constraint is the restricted context provided by these interfaces, making it challenging to grasp the full scope and intricacies of the code under examination. Moreover, concerns regarding data privacy and security loom large, as sensitive codebases may contain proprietary or confidential information that cannot be safely processed on remote servers.\n",
    "\n",
    "By leveraging the power of Retrieval-Augmented Generation (RAG) combined with a model running locally, these concerns can be at least alleviated.\n",
    "Indeed, this approach could help developers navigating through complex codebases with greater efficiency and confidence, while preserving data privacy and security. \n",
    "\n",
    "\n",
    "**Tech Stack**\n",
    "\n",
    "- LangChain (Application Framework)\n",
    "- HuggingFace SentenceTransformers (Text Embedding)\n",
    "- FAISS (Vector Store)\n",
    "- Ollama (Local LLMs Platform)\n",
    "- Gemma:7b (LLM)\n",
    "\n",
    "**Sources** \n",
    "\n",
    "Most of the content of this notebook follow along the documentation and example pages of [LangChain documentation](https://python.langchain.com/docs/get_started/introduction). We tried gathering and integrating further details for a novice of the LLM landscape. Please refer to the linked original sources whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the [RAG Architecture ](https://python.langchain.com/docs/use_cases/question_answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG models combine the strengths of both retrieval-based and generation-based approaches. By retrieving relevant context from a large corpus of documents before generating a response, RAG models can produce more relevant and updated outputs compared to generation-only models. Indeed, recall that all LLMs have a cutoff date in their training data. \n",
    "\n",
    "A typical RAG application consists of two main components:\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "Indexing involves a pipeline for ingesting data from a source and indexing it. This process typically occurs offline.\n",
    "\n",
    "1. **Load**: The first step is to load the data from the textual files.\n",
    "2. **Split**: Text splitters divide large documents into smaller chunks, which is beneficial for both indexing data and passing it to a model. This is because large chunks are harder to search over and may not fit within a model's finite context window.\n",
    "3. **Store**: It's necessary to have a place to store and index the splits so that they can be later searched over. This is often accomplished using a Vector Store and an Embedding model.\n",
    "\n",
    "\n",
    "\n",
    "**Retrieval and Generation**\n",
    "\n",
    "This phase involves the actual use of the RAG chain, which takes the user query at runtime and retrieves relevant data from the index, then passes it to the model.\n",
    "\n",
    "1. **Retrieve**: Relevant splits are retrieved from storage based on user input, using a Retriever.\n",
    "2. **Generate**: An LLM produces an answer using a prompt that includes both the question and the retrieved data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents loading and splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To load source code contents, we use DocumentLoaders, which return a list of Documents with content and metadata.\n",
    "- Indexing involves splitting loaded documents into chunks for embedding and vector storage, aiding retrieval efficiency.\n",
    "- LanguageParser is useful because it effectively organizes the source code, maintains logical groupings of functions and classes, separates code elements for clarity, and preserves important metadata for context and traceability.\n",
    "- Documents are split into 8000-character chunks, since Documents may not fit in the context window of many models. The 200 characters of overlap using RecursiveCharacterTextSplitter help to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "folder_name = \"/Users/antoniocusano/Coding/WebScraper\"\n",
    "language = Language.PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    folder_name,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=language, parser_threshold=1000),\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=language, chunk_size=10_000, chunk_overlap=800\n",
    ")\n",
    "texts = code_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 documente loaded (12 chunks after split).\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(documents)} documente loaded ({len(texts)} chunks after split).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Index generation/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to index our text chunks so that we can search over them at runtime.\n",
    "- We can embed the contents of each document split and insert these embeddings into a vector database (or vector store). \n",
    "- To search over our splits, we take a text search query, embed it, and perform a similarity search (e.g. cosine similarity) to identify the stored splits with the most similar embeddings to our query embedding.\n",
    "- Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings generation\n",
    "embeddings= HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2')\n",
    "db = FAISS.from_documents(texts,embeddings)\n",
    "db.save_local(\"./embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings loading\n",
    "embeddings= HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2')\n",
    "db = FAISS.load_local(\"embeddings\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Sample retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "- VectorStoreRetriever uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever()\n",
    "- You can specify the number $k$ of most relevant documents to retrieve\n",
    "- By default, the vector store retriever uses similarity search, but you can specify maximum marginal relevance search as the search type if supported.\n",
    "- The MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import subprocess\n",
      "import sys\n",
      "import os\n",
      "from datetime import datetime\n",
      "from apscheduler.schedulers.blocking import BlockingScheduler\n",
      "\n",
      "\n",
      "def get_scraper_path():\n",
      "    return os.path.join(\".\", \"usedcars_scraper.py\")\n",
      "\n",
      "\n",
      "def get_python_command():\n",
      "    return sys.executable\n",
      "\n",
      "\n",
      "def run_scraper(python_command, scraper_path):\n",
      "    print(f\"Scraper running at {datetime.now()}\")\n",
      "    subprocess.run([python_command, scraper_path], check=True)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"Scraper scheduler started\")\n",
      "    python_command = get_python_command()\n",
      "    scraper_path = get_scraper_path()\n",
      "\n",
      "    sched = BlockingScheduler()\n",
      "    sched.add_job(\n",
      "        run_scraper,\n",
      "        \"interval\",\n",
      "        hours=1,\n",
      "        next_run_time=datetime.now(),\n",
      "        args=[python_command, scraper_path],\n",
      "    )\n",
      "\n",
      "    try:\n",
      "        sched.start()\n",
      "    except (KeyboardInterrupt, SystemExit):\n",
      "        sched.shutdown(wait=False)\n"
     ]
    }
   ],
   "source": [
    "query = \"What do the scraper do?\"\n",
    "docs_retrieved = retriever.invoke(query)\n",
    "print(docs_retrieved[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a local large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LangChain, there are two main components of language models: Large Language Models (LLMs) and Chat Models, each serving distinct purposes\n",
    "- LLMs are more general-purpose, they take a string as input and return a string as output. \n",
    "- Chat Models are specialized for conversational tasks, they take a list of messages as input and return a chat message\n",
    "- Notice that LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs\n",
    "- LangChain has integrations with many open-source LLMs that can be run locally. Here we use the Ollama integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Ollama](https://ollama.com/) is a free tool to run open-source large language models locally\n",
    "- Examples of model (families) that can be run in ollama include Gemma, Llama 2, mistral...\n",
    "- To interact with models via Ollama, users can run commands like ```ollama run <name-of-model>``` to chat directly with a model from the command line\n",
    "- They can also send prompts to the models via a [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md):  ```curl http://localhost:11434/api/generate -d '{\"model\": \"gemma:7b\",\"prompt\":\"Tell me a joke\"}' ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the tomato turn red?\\n\\nBecause it saw the salad dressing.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma:7b\")\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A prompt template is a piece of text to complete with an input to and produces a prompt for a language model.\n",
    "- One can use LangChain Prompt Hub to store and fetch prompts or use the class PromptTemplate\n",
    "- rlm/rag-prompt is a prompt template for retrieval-augmented-generation. It is useful for chat, QA, or other applications that rely on passing context to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rlm/rag-prompt** \n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. \\\n",
    "\\\n",
    "Question: {question} \\\n",
    "Context: {context} \\\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Sample prompt\n",
    "prompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a Q&A with retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LangChain, a \"chain\" is a fundamental component that forms the backbone of the platform's functionality. They are essentially sequences of components executed in a specific order to a task.\n",
    "- There are two types of off-the-shelf chains that LangChain supports:\n",
    "    - Chains that are built with LangChain Expression Language (LCEL), e.g. ```chain = prompt | model | output_parser```\n",
    "    - Chains constructed by subclassing from a legacy Chain class, e.g. ```qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt) ```\n",
    "- Here we put together in a chain all our ingredients: the retriever, the llm and the prompt template\n",
    "- We add an output parser to transform the output of the LLM into a more readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generation of Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pipeline is complete and the question can be asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The code below calculates the price of a car based on its specifications. The price is then displayed in the output.\\n\\n```python\\n# Calculate the price of a car\\n\\n# Define the parameters\\ncar_make = \"Toyota\"\\ncar_model = \" sedan\"\\ncar_year = 2019\\ncar_mileage = 100\\ncar_price = \\n\\n# Calculate the price\\nprice = (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage))\\n\\n# Display the price\\nprint(f\"The price of the car is $ {price}\")\\n```\\n\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the attributes of the cars collected?\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
