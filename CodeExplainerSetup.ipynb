{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Understanding with Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides insights into building a Code Understanding App using Local LLMs and a RAG architecture. \\\n",
    "*Please notice that the notebook is still being edited.*\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "In the realm of software development, grappling with ill documented or poorly written code is a common challenge that consumes valuable time and resources. \n",
    "\n",
    "Despite source code analysis being one of the most popular LLM applications, traditional web-based interfaces come with their own set of limitations.\n",
    "One primary constraint is the restricted context provided by these interfaces, making it challenging to grasp the full scope and intricacies of the code under examination. Moreover, concerns regarding data privacy and security loom large, as sensitive codebases may contain proprietary or confidential information that cannot be safely processed on remote servers.\n",
    "\n",
    "By leveraging the power of Retrieval-Augmented Generation (RAG) combined with a model running locally, these concerns can be at least alleviated.\n",
    "Indeed, this approach could help developers navigating through complex codebases with greater efficiency and confidence, while preserving data privacy and security. \n",
    "\n",
    "\n",
    "**Tech Stack**\n",
    "\n",
    "- LangChain (Application Framework)\n",
    "- HuggingFace SentenceTransformers (Text Embedding)\n",
    "- FAISS (Vector Store)\n",
    "- Ollama (Local LLMs Platform)\n",
    "- Gemma:7b (LLM)\n",
    "\n",
    "**Sources** \n",
    "\n",
    "Most of the content of this notebook follow along the documentation and example pages of [LangChain documentation](https://python.langchain.com/docs/get_started/introduction). We tried gathering and integrating further details for a novice of the LLM landscape. Please refer to the linked original sources whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the [RAG Architecture ](https://python.langchain.com/docs/use_cases/question_answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG models combine the strengths of both retrieval-based and generation-based approaches. By retrieving relevant context from a large corpus of documents before generating a response, RAG models can produce more relevant and updated outputs compared to generation-only models. Indeed, recall that all LLMs have a cutoff date in their training data. \n",
    "\n",
    "A typical RAG application consists of two main components:\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "Indexing involves a pipeline for ingesting data from a source and indexing it. This process typically occurs offline.\n",
    "\n",
    "1. **Load**: The first step is to load the data from the textual files.\n",
    "2. **Split**: Text splitters divide large documents into smaller chunks, which is beneficial for both indexing data and passing it to a model. This is because large chunks are harder to search over and may not fit within a model's finite context window.\n",
    "3. **Store**: It's necessary to have a place to store and index the splits so that they can be later searched over. This is often accomplished using a Vector Store and an Embedding model.\n",
    "\n",
    "\n",
    "\n",
    "**Retrieval and Generation**\n",
    "\n",
    "This phase involves the actual use of the RAG chain, which takes the user query at runtime and retrieves relevant data from the index, then passes it to the model.\n",
    "\n",
    "1. **Retrieve**: Relevant splits are retrieved from storage based on user input, using a Retriever.\n",
    "2. **Generate**: An LLM produces an answer using a prompt that includes both the question and the retrieved data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents loading and splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To load source code contents, we use `DocumentLoaders`, which return a list of `Documents` with content and metadata\n",
    "- `glob=\"**/*\"` includes the folder in path and all subfolders\n",
    "- `LanguageParser` class of LangChain is useful because it effectively organizes the source code, maintains logical groupings of functions and classes, separates code elements for clarity, and preserves important metadata for context and traceability.\n",
    "- The `parser_threshold` is used to set the minimum number of lines required to activate the splitting based on syntax when parsing code using the respective programming language syntax. This parameter allows for more precise segmentation of code by loading each top-level function and class into separate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "folder_name = \"/Users/antoniocusano/Coding/WebScraper\"\n",
    "language = Language.PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    path=folder_name,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=language, parser_threshold=10),\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documents are split into chunks, since the documents may not fit in the context window of many models.\n",
    "- `RecursiveCharacterTextSplitter` splits text by recursively look at a list of separator characters \n",
    "- For Python language, first it tries to split along class definitions (`class`, `def`, ...) then split by the normal type of line (`\\n`, `\" \"`, ...)\n",
    "- The characters of overlap help to preserve context.\n",
    "- Notice that there are at least two limits that must be taken into account: the context length of the LLM (at inference time) and the max sequence length of the embedding model (at training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 documente loaded (77 chunks after split).\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=language, chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "doc_chunks = code_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(documents)} documente loaded ({len(doc_chunks)} chunks after split).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document chunks: \n",
      "\n",
      "import streamlit as st\n",
      "\n",
      "st.set_page_config(\n",
      "    pa\n",
      "def filter_usedcars_data(\n",
      "    df, max_price=1e6, m\n",
      "def load_data(nrows: int):\n",
      "    columns = [\n",
      "       \n",
      "def load_model():\n",
      "    model_path = os.path.join(\".\n",
      "def make_prediction(inputs: dict):\n",
      "    prediction \n"
     ]
    }
   ],
   "source": [
    "print(\"Sample document chunks: \\n\")\n",
    "for ii in range(5):\n",
    "    print(doc_chunks[ii].page_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected max number of tokens per document chunk \n",
    "# 5 characters per word, 0.75 word per token -> 4 characters per token\n",
    "int(max([len(sent.page_content.split())*4/3 for sent in doc_chunks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Index generation/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to index our text chunks so that we can search over them at runtime.\n",
    "- We can embed the contents of each document split and insert these embeddings into a vector database (or vector store). \n",
    "- To search over our splits, we take a text search query, embed it, and perform a similarity search (e.g. cosine similarity) to identify the stored splits with the most similar embeddings to our query embedding.\n",
    "- As pointed out in [HuggingFace Documentation](https://huggingface.co/learn/cookbook/advanced_rag), it is important to check the embedding sequence length limit\n",
    "- Longer sequences may be truncated without a warning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's maximum sequence length: 256\n",
      "Doc base maximum sequence length: 457\n",
      "Doc base median sequence length: 204\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Model's maximum sequence length: {SentenceTransformer(embedding_model_name).get_max_seq_length()}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in doc_chunks]\n",
    "\n",
    "print(f\"Doc base maximum sequence length: {np.max(lengths)}\")\n",
    "print(f\"Doc base median sequence length: {int(np.median(lengths))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Facebook AI Similarity Search ([FAISS](https://faiss.ai/)) is a library for efficient similarity search and clustering of dense vectors\n",
    "- The defualt similarity measure is euclidean distance in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings generation\n",
    "embeddings= HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "vector_db = FAISS.from_documents(doc_chunks,embeddings)\n",
    "vector_db.save_local(\"./embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings loading\n",
    "embeddings= HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "vector_db = FAISS.load_local(\"embeddings\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Sample retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query.\n",
    "- `VectorStoreRetriever` uses the similarity search capabilities of a vector store to facilitate retrieval. Any `VectorStore` can easily be turned into a Retriever with `VectorStore.as_retriever()`\n",
    "- You can specify the number $k$ of most relevant documents to retrieve\n",
    "- By default, the vector store retriever uses similarity search, but you can specify maximum marginal relevance search as the search type if supported.\n",
    "- The `MaxMarginalRelevanceExampleSelector` selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_scraper_path():\n",
      "    return os.path.join(\".\", \"usedcars_scraper.py\")\n"
     ]
    }
   ],
   "source": [
    "query = \"What do the scraper do?\"\n",
    "docs_retrieved = retriever.invoke(query)\n",
    "print(docs_retrieved[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a local large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LangChain, there are two main components of language models: Large Language Models (LLMs) and Chat Models, each serving distinct purposes\n",
    "- LLMs are more general-purpose, they take a string as input and return a string as output. \n",
    "- Chat Models are specialized for conversational tasks, they take a list of messages as input and return a chat message\n",
    "- Notice that LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs\n",
    "- LangChain has integrations with many open-source LLMs that can be run locally. Here we use the Ollama integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Ollama](https://ollama.com/) is a free tool to run open-source large language models locally\n",
    "- Examples of model (families) that can be run in ollama include Gemma, Llama 2, mistral...\n",
    "- To interact with models via Ollama, users can run commands like ```ollama run <name-of-model>``` to chat directly with a model from the command line\n",
    "- They can also send prompts to the models via a [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md):  ```curl http://localhost:11434/api/generate -d '{\"model\": \"gemma:7b\",\"prompt\":\"Tell me a joke\"}' ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the tomato turn red?\\n\\nBecause it saw the salad dressing.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma:7b\")\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A prompt template is a piece of text to complete with an input to and produces a prompt for a language model.\n",
    "- One can use LangChain Prompt Hub to store and fetch prompts or use the class PromptTemplate\n",
    "- rlm/rag-prompt is a prompt template for retrieval-augmented-generation. It is useful for chat, QA, or other applications that rely on passing context to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rlm/rag-prompt** \n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. \\\n",
    "\\\n",
    "Question: {question} \\\n",
    "Context: {context} \\\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Sample prompt\n",
    "prompt.invoke({\"context\": \"filler context\", \"question\": \"filler question\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of a Q&A with retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LangChain, a \"chain\" is a fundamental component that forms the backbone of the platform's functionality. They are essentially sequences of components executed in a specific order to a task.\n",
    "- There are two types of off-the-shelf chains that LangChain supports:\n",
    "    - Chains that are built with LangChain Expression Language (LCEL), e.g. ```chain = prompt | model | output_parser```\n",
    "    - Chains constructed by subclassing from a legacy Chain class, e.g. ```qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt) ```\n",
    "- Here we put together in a chain all our ingredients: the retriever, the llm and the prompt template\n",
    "- We add an output parser to transform the output of the LLM into a more readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generation of Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pipeline is complete and the question can be asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The code below calculates the price of a car based on its specifications. The price is then displayed in the output.\\n\\n```python\\n# Calculate the price of a car\\n\\n# Define the parameters\\ncar_make = \"Toyota\"\\ncar_model = \" sedan\"\\ncar_year = 2019\\ncar_mileage = 100\\ncar_price = \\n\\n# Calculate the price\\nprice = (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage)) + (100 * (0.12 * car_mileage))\\n\\n# Display the price\\nprint(f\"The price of the car is $ {price}\")\\n```\\n\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the attributes of the cars collected?\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
